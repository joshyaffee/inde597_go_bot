{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.actions = [\"RIGHT\", \"UP\", \"DOWN\", \"LEFT\"]\n",
    "        self.max_x = 12\n",
    "        self.max_y = 3\n",
    "        self.start = [0, 0]\n",
    "        self.goal = [self.max_x, 0]\n",
    "        self.current = self.start\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        if action == \"LEFT\":\n",
    "            self.current[0] -= 1\n",
    "        elif action == \"RIGHT\":\n",
    "            self.current[0] += 1\n",
    "        elif action == \"UP\":\n",
    "            self.current[1] += 1\n",
    "        elif action == \"DOWN\":\n",
    "            self.current[1] -= 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "        \n",
    "        if self.current[0] < 0:\n",
    "            self.current[0] = 0\n",
    "        elif self.current[0] > self.max_x:\n",
    "            self.current[0] = self.max_x\n",
    "    \n",
    "        if self.current[1] < 0:\n",
    "            self.current[1] = 0\n",
    "        elif self.current[1] > self.max_y:\n",
    "            self.current[1] = self.max_y\n",
    "\n",
    "        if self.current[1] == 0 and self.current[0] not in (0, self.max_x):\n",
    "            self.current = [0, 0]\n",
    "            return self.current, -100, False\n",
    "\n",
    "        if self.current == self.goal:\n",
    "            return self.current, 0, True\n",
    "        else:\n",
    "            return self.current, -1, False\n",
    "        \n",
    "    def reset(self, state = None):\n",
    "        if state is not None:\n",
    "            self.current = state.copy()\n",
    "        else:\n",
    "            self.current = self.start.copy()\n",
    "        return self.current\n",
    "    \n",
    "    def render(self):\n",
    "        for y in range(self.max_y, -1, -1):\n",
    "            for x in range(self.max_x + 1):\n",
    "                if [x, y] == self.current:\n",
    "                    print(\"X\", end = \" \")\n",
    "                elif [x, y] == self.start:\n",
    "                    print(\"S\", end = \" \")\n",
    "                elif [x, y] == self.goal:\n",
    "                    print(\"G\", end = \" \")\n",
    "                else:\n",
    "                    print(\".\", end = \" \")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    super class for Q-learning and SARSA agents\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.actions = env.actions\n",
    "        self.init_q_table()\n",
    "\n",
    "    def init_q_table(self):\n",
    "        \"\"\"initial q should be arbitrary value for nonterminal (s,a) and 0 for terminal\n",
    "        (s,.)\"\"\"\n",
    "        self.q_table = np.ones((self.env.max_x + 1, self.env.max_y + 1, len(self.actions)))\n",
    "        self.q_table[self.env.goal[0], self.env.goal[1], :] = 0\n",
    "\n",
    "    def epsilon_greedy(self, state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(self.actions)\n",
    "        else:\n",
    "            return self.actions[np.argmax(self.q_table[state[0], state[1], :])]\n",
    "        \n",
    "    def train(self, episodes, epsilon, alpha, gamma):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def test(self, num_episodes = 1000, epsilon = 0):\n",
    "        \"\"\"\n",
    "        test the learned policy\n",
    "        \"\"\"\n",
    "        _returns = []\n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            _return = 0\n",
    "            while not done:\n",
    "                action = self.epsilon_greedy(state, epsilon)\n",
    "                state, reward, done = self.env.step(action)\n",
    "                _return += reward\n",
    "            _returns.append(_return)\n",
    "\n",
    "        return np.mean(_returns), np.std(_returns)\n",
    "        \n",
    "    def disp(self):\n",
    "        \"\"\"display the agent's policy with unicode arrows\"\"\"\n",
    "        for y in range(self.env.max_y, -1, -1):\n",
    "            for x in range(self.env.max_x + 1):\n",
    "                if [x, y] == self.env.goal:\n",
    "                    print(\"G\", end = \" \")\n",
    "                else:\n",
    "                    # if there's a tie, show all actions\n",
    "                    if np.sum(self.q_table[x, y, :] == np.max(self.q_table[x, y, :])) > 1:\n",
    "                        print(\"?\", end = \" \")\n",
    "                    else:\n",
    "                        action = self.actions[np.argmax(self.q_table[x, y, :])]\n",
    "                        if action == \"LEFT\":\n",
    "                            print(\"←\", end = \" \")\n",
    "                        elif action == \"RIGHT\":\n",
    "                            print(\"→\", end = \" \")\n",
    "                        elif action == \"UP\":\n",
    "                            print(\"↑\", end = \" \")\n",
    "                        elif action == \"DOWN\":\n",
    "                            print(\"↓\", end = \" \")\n",
    "            print()  \n",
    "        print()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(Agent):\n",
    "    \"\"\"\n",
    "    off policy TD-learner\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self, num_episodes, epsilon = .1, alpha = .5 , gamma = .9):\n",
    "        _returns = []\n",
    "        for _ in tqdm.tqdm(range(num_episodes)):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            _return = 0\n",
    "            i = 0\n",
    "            while not done:\n",
    "                action = self.epsilon_greedy(state, epsilon)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                _return += reward\n",
    "                self.q_table[state[0], state[1], self.actions.index(action)] += alpha * (reward + gamma * np.max(self.q_table[next_state[0], next_state[1], :]) - self.q_table[state[0], state[1], self.actions.index(action)])\n",
    "                state = next_state\n",
    "                i += 1\n",
    "                if i % 1_000_000 == 0:\n",
    "                    print(i)\n",
    "                    self.disp()\n",
    "            _returns.append(_return)\n",
    "        return _returns\n",
    "    \n",
    "class Sarsa(Agent):\n",
    "    \"\"\"\n",
    "    on policy TD-learner\n",
    "    \"\"\"\n",
    "\n",
    "    def train(self, num_episodes, epsilon = .1, alpha = .5 , gamma = .9):\n",
    "        _returns = []\n",
    "        for episode in tqdm.tqdm(range(num_episodes)):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            action = self.epsilon_greedy(state, epsilon)\n",
    "            _return = 0\n",
    "            i = 0\n",
    "            while not done:\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                _return += reward\n",
    "                next_action = self.epsilon_greedy(next_state, epsilon)\n",
    "                self.q_table[state[0], state[1], self.actions.index(action)] += alpha * (reward + gamma * self.q_table[next_state[0], next_state[1], self.actions.index(next_action)] - self.q_table[state[0], state[1], self.actions.index(action)])\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                i += 1\n",
    "                if i % 1_000_000 == 0:\n",
    "                    print(i)\n",
    "                    self.disp()\n",
    "            _returns.append(_return)\n",
    "        return _returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_returns():\n",
    "    env = Env()\n",
    "    sarsa = Sarsa(env)\n",
    "    q_learning = QLearning(env)\n",
    "    num_episodes = 500\n",
    "    q_learning_returns = q_learning.train(num_episodes)\n",
    "    sarsa_returns = sarsa.train(num_episodes)\n",
    "    \n",
    "    sns.lineplot(x = range(num_episodes), y = sarsa_returns, label = \"SARSA\")\n",
    "    sns.lineplot(x = range(num_episodes), y = q_learning_returns, label = \"Q-learning\")\n",
    "    plt.ylim(-250, 0)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Return\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/500 [00:17<10:02,  1.23s/it]"
     ]
    }
   ],
   "source": [
    "plot_returns()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
